{"version":3,"sources":["worker/file-Indexing-Worker-thread.js"],"names":["urlParams","URLSearchParams","location","search","debugMode","get","Phoenix","baseURL","importScripts","virtualfs","console","log","info","projectCache","files","currentCrawlID","MAX_FILE_SIZE_TO_INDEX","currentCrawlIndex","crawlComplete","crawlEventSent","cacheStartTime","Date","now","cacheSize","clearProjectCache","_statAsync","path","Promise","resolve","reject","fs","stat","err","stats","_readFileAsync","readFile","data","async","getFilesizeInBytes","fileName","size","ex","getFileContentsForFile","filePath","fileSize","fileCrawler","crawlerID","length","setTimeout","parallelRead","readPromises","crawlingPaths","i","push","contents","all","content","e","error","crawlTime","WorkerComm","triggerPeer","numFilesCached","cacheSizeBytes","crawlTimeMs","_crawlProgressMessenger","processed","total","initCache","fileList","removeFilesFromCache","updateObject","filesInSearchScope","isNotInRemovedFilesList","indexOf","filter","addFilesToCache","changedFilesAlreadyInList","newFiles","isInChangedFileList","isNotAlreadyInList","apply","documentChanged","docContents","setTauriWsFS","nodeWSURL","setNodeWSEndpoint","forceUseNodeWSEndpoint","setInterval","setExecHandler"],"mappings":"AAuBA,MAAMA,UAAY,IAAIC,gBAAgBC,SAASC,QACzCC,UAAwC,SAA3BJ,UAAUK,IAAI,SAE3BC,QAAU,CAEZC,QAAS,OAEbC,cAAc,2BACdA,cAAc,+BACdA,cAAc,mBAEdC,UAAUL,UAAYA,UAEtBM,QAAQC,IAAI,8CAA+CP,WAEvDA,YACAM,QAAQC,IAAMD,QAAQE,KAAO,cAKjC,IAAIC,aAAe,GACfC,MACAC,eAAiB,EACjBC,uBAAyB,SAEzBC,kBAAoB,EACpBC,eAAgB,EAChBC,gBAAiB,EACjBC,eAAiBC,KAAKC,MACtBC,UAAY,EAKhB,SAASC,oBACLX,aAAe,GAGnB,SAASY,WAAWC,MAChB,OAAO,IAAIC,QAAQ,CAACC,QAASC,UACzBC,GAAGC,KAAKL,KAAM,SAAUM,IAAKC,OACrBD,IACAH,OAAOG,KAEPJ,QAAQK,WAMxB,SAASC,eAAeR,MACpB,OAAO,IAAIC,QAAQ,CAACC,QAASC,UACzBC,GAAGK,SAAST,KAAM,OAAQ,SAAUM,IAAKI,MACjCJ,IACAH,OAAOG,KAEPJ,QAAQQ,UAYxBC,eAAeC,mBAAmBC,UAC9B,IACI,IAAIN,MACJ,aADkBR,WAAWc,WAChBC,MAAQ,EACvB,MAAOC,IAEL,OADA/B,QAAQC,IAAI8B,IACL,GAUfJ,eAAeK,uBAAuBC,UAClC,GAAI9B,aAAa8B,WAAwC,KAA3B9B,aAAa8B,UACvC,OAAO9B,aAAa8B,UAExB,IACI,IAAIC,eAAiBN,mBAAmBK,UAEpC9B,aAAa8B,UADZC,UAAY5B,6BACkBkB,eAAeS,UAErB,GAE/B,MAAOF,IACL/B,QAAQC,IAAI8B,IACZ5B,aAAa8B,UAAY,GAE7B,OAAO9B,aAAa8B,UASxBN,eAAeQ,YAAYC,WACvB,IAAKhC,OAAUA,OAA0B,IAAjBA,MAAMiC,OAE1B,YADAC,WAAW,IAAIH,YAAYC,WAAY,KAG3C,MAAMG,aAAe,EACrB,IAAIC,aAAe,GACfC,cAAgB,GACpB,IAAK,IAAIC,EAAI,EAAGA,EAHK,GAGenC,kBAAoBH,MAAMiC,OAAQK,IAClED,cAAcE,KAAKvC,MAAMG,oBACzBiC,aAAaG,KAAKX,uBAAuB5B,MAAMG,qBAC/CA,oBAEJ,IACI,IAAIqC,eAAiB3B,QAAQ4B,IAAIL,eAAiB,GAClD,IAAI,IAAIM,WAAWF,SACZE,SAAWA,QAAQT,SAClBxB,WAAaiC,QAAQT,QAG/B,MAAOU,GACL/C,QAAQgD,MAAM,4CAA6CP,cAAeM,GAE9E,GAAG1C,iBAAmB+B,UAKtB,GAAI7B,kBAAoBH,MAAMiC,OAC1B7B,eAAgB,EAChB8B,WAAW,IAAIH,YAAYC,gBACxB,CAEH,GADA5B,eAAgB,GACXC,eAAgB,CACjBA,gBAAiB,EACjB,IAAIwC,UAAatC,KAAKC,MAAQF,eAC9BwC,WAAWC,YAAY,gBAAiB,CACpCC,eAAgBhD,MAAMiC,OACtBgB,eAAgBxC,UAChByC,YAAaL,YAGrBX,WAAW,IAAIH,YAAYC,WAAY,MAI/C,SAASmB,2BACD/C,eAAiBJ,OACjB8C,WAAWC,YAAY,gBAAiB,CACpCK,UAAWjD,kBACXkD,MAAOrD,MAAMiC,SAYzB,SAASqB,UAAUC,UACf3D,QAAQC,IAAI,2CAA4C0D,SAAStB,QACjEjC,MAAQuD,SACRpD,kBAAoB,EACpBM,UAAY,EACZC,oBACAL,gBAAiB,EACjBC,eAAiBC,KAAKC,MAEtBuB,YADA9B,gBAAkC,GAElC6C,WAAWC,YAAY,gBACH,IAAjB/C,MAAMiC,QACLa,WAAWC,YAAY,gBAAiB,CACpCC,eAAgBhD,MAAMiC,OACtBgB,eAAgB,EAChBC,YAAa,IASzB,SAASM,qBAAqBC,cAC1B7D,QAAQC,IAAI,gCAAiC4D,cAC7C,IAAIF,SAAWE,aAAaF,UAAY,GACpCG,mBAAqBD,aAAaC,oBAAsB,GACxDpB,EAAI,EACR,IAAKA,EAAI,EAAGA,EAAIiB,SAAStB,OAAQK,WACtBvC,aAAawD,SAASjB,IAEjC,SAASqB,wBAAwB/C,MAC7B,OAA8C,IAAtC8C,mBAAmBE,QAAQhD,MAEvCZ,MAAQA,MAAQA,MAAM6D,OAAOF,yBAA2B3D,MAQ5D,SAAS8D,gBAAgBL,cACrB7D,QAAQC,IAAI,4BAA6B4D,cACzC,IAAIF,SAAWE,aAAaF,UAAY,GACpCG,mBAAqBD,aAAaC,oBAAsB,GACxDpB,EAAI,EACJyB,0BAA4B,GAC5BC,SAAW,GACf,IAAK1B,EAAI,EAAGA,EAAIiB,SAAStB,OAAQK,IAG7BvC,aAAawD,SAASjB,IAAM,KAIhC,SAAS2B,oBAAoBrD,MACzB,OAA8C,IAAtC8C,mBAAmBE,QAAQhD,MAGvC,SAASsD,mBAAmBtD,MACxB,OAAqD,IAA7CmD,0BAA0BH,QAAQhD,MAE9CoD,UAJAD,0BAA4B/D,MAAQA,MAAM6D,OAAOI,qBAAuB,IAInCJ,OAAOK,oBAC5ClE,MAAMuC,KAAK4B,MAAMnE,MAAOgE,UAO5B,SAASI,gBAAgBX,cACrB7D,QAAQC,IAAI,mBAAoB4D,cAChC1D,aAAa0D,aAAa5B,UAAY4B,aAAaY,YAGvD,SAASC,aAAaC,WAClBvD,GAAGwD,kBAAkBD,WACrBvD,GAAGyD,wBAAuB,GAtF9BC,YAAYvB,wBAAyB,KAyFrCL,WAAW6B,eAAe,eAAgBL,cAC1CxB,WAAW6B,eAAe,YAAarB,WACvCR,WAAW6B,eAAe,eAAgBb,iBAC1ChB,WAAW6B,eAAe,kBAAmBP,iBAC7CtB,WAAW6B,eAAe,eAAgBnB","sourcesContent":["/*\n * GNU AGPL-3.0 License\n *\n * Copyright (c) 2021 - present core.ai . All rights reserved.\n * Original work Copyright (c) 2015 - 2021 Adobe Systems Incorporated. All rights reserved.\n *\n * This program is free software: you can redistribute it and/or modify it\n * under the terms of the GNU Affero General Public License as published by\n * the Free Software Foundation, either version 3 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful, but WITHOUT\n * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n * FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License\n * for more details.\n *\n * You should have received a copy of the GNU Affero General Public License\n * along with this program. If not, see https://opensource.org/licenses/AGPL-3.0.\n *\n */\n\n/*global virtualfs, fs, WorkerComm */\n\nconst urlParams = new URLSearchParams(location.search);\nconst debugMode = (urlParams.get('debug') === 'true');\n// eslint-disable-next-line no-unused-vars\nconst Phoenix = {\n    // exported to be used by extensions that extend the indexing worker\n    baseURL: '../'\n};\nimportScripts('../phoenix/virtualfs.js');\nimportScripts('../utils/EventDispatcher.js');\nimportScripts('./WorkerComm.js');\n\nvirtualfs.debugMode = debugMode;\n\nconsole.log(\"File indexing worker loaded in debug mode: \", debugMode);\n\nif(!debugMode){\n    console.log = console.info = function () {\n        // swallow logs\n    };\n}\n\nlet projectCache = [],\n    files,\n    currentCrawlID = 0,\n    MAX_FILE_SIZE_TO_INDEX = 16777216;\n\nlet currentCrawlIndex = 0,\n    crawlComplete = false,\n    crawlEventSent = false,\n    cacheStartTime = Date.now(),\n    cacheSize = 0;\n\n/**\n * Clears the cached file contents of the project\n */\nfunction clearProjectCache() {\n    projectCache = [];\n}\n\nfunction _statAsync(path) {\n    return new Promise((resolve, reject)=>{\n        fs.stat(path, function (err, stats) {\n            if (err) {\n                reject(err);\n            } else {\n                resolve(stats);\n            }\n        });\n    });\n}\n\nfunction _readFileAsync(path) {\n    return new Promise((resolve, reject)=>{\n        fs.readFile(path, 'utf8', function (err, data) {\n            if (err) {\n                reject(err);\n            } else {\n                resolve(data);\n            }\n        });\n    });\n}\n\n\n/**\n * Gets the file size in bytes.\n * @param   {string} fileName The name of the file to get the size\n * @returns {Number} the file size in bytes\n */\nasync function getFilesizeInBytes(fileName) {\n    try {\n        let stats = await _statAsync(fileName);\n        return stats.size || 0;\n    } catch (ex) {\n        console.log(ex);\n        return 0;\n    }\n}\n\n/**\n * Get the contents of a file from cache given the path. Also adds the file contents to cache from disk if not cached.\n * Will not read/cache files greater than MAX_FILE_SIZE_TO_INDEX in size.\n * @param   {string} filePath full file path\n * @return {string} contents or null if no contents\n */\nasync function getFileContentsForFile(filePath) {\n    if (projectCache[filePath] || projectCache[filePath] === \"\") {\n        return projectCache[filePath];\n    }\n    try {\n        let fileSize = await getFilesizeInBytes(filePath);\n        if ( fileSize <= MAX_FILE_SIZE_TO_INDEX) {\n            projectCache[filePath] = await _readFileAsync(filePath);\n        } else {\n            projectCache[filePath] = \"\";\n        }\n    } catch (ex) {\n        console.log(ex);\n        projectCache[filePath] = \"\";\n    }\n    return projectCache[filePath];\n}\n\n/**\n * Crawls through the files in the project and stores them in cache. Since that could take a while\n * we do it in batches so that node won't be blocked.\n * @param {string|number} crawlerID a unique id for the crawl to start the crawler. If the global currentCrawlID\n *    changes, the file crawler will stop and another should be scheduled by the initCache fn.\n */\nasync function fileCrawler(crawlerID) {\n    if (!files || (files && files.length === 0)) {\n        setTimeout(()=>fileCrawler(crawlerID), 1000);\n        return;\n    }\n    const parallelRead = 5;\n    let readPromises = [];\n    let crawlingPaths = [];\n    for (let i = 0; i < parallelRead && currentCrawlIndex < files.length; i++) {\n        crawlingPaths.push(files[currentCrawlIndex]);\n        readPromises.push(getFileContentsForFile(files[currentCrawlIndex]));\n        currentCrawlIndex++;\n    }\n    try{\n        let contents = await Promise.all(readPromises) || [];\n        for(let content of contents){\n            if(content && content.length){\n                cacheSize += content.length;\n            }\n        }\n    } catch (e) {\n        console.error(`Something went wrong when indexing paths:`, crawlingPaths, e);\n    }\n    if(currentCrawlID !== crawlerID) {\n        // The project to crawl switched while we were waiting for previous crawl data.\n        // So stop the current crawler as another crawler will be scheduled by the initCache fn.\n        return;\n    }\n    if (currentCrawlIndex < files.length) {\n        crawlComplete = false;\n        setTimeout(()=>fileCrawler(crawlerID));\n    } else {\n        crawlComplete = true;\n        if (!crawlEventSent) {\n            crawlEventSent = true;\n            let crawlTime =  Date.now() - cacheStartTime;\n            WorkerComm.triggerPeer(\"crawlComplete\", {\n                numFilesCached: files.length,\n                cacheSizeBytes: cacheSize,\n                crawlTimeMs: crawlTime\n            });\n        }\n        setTimeout(()=>fileCrawler(crawlerID), 1000);\n    }\n}\n\nfunction _crawlProgressMessenger() {\n    if(!crawlComplete && files){\n        WorkerComm.triggerPeer(\"crawlProgress\", {\n            processed: currentCrawlIndex,\n            total: files.length\n        });\n    }\n}\n\nsetInterval(_crawlProgressMessenger, 1000);\n\n/**\n * Init for project, resets the old project cache, and sets the crawler function to\n * restart the file crawl\n * @param   {array} fileList an array of files\n */\nfunction initCache(fileList) {\n    console.log(\"file indexer: InitCache with num files: \", fileList.length);\n    files = fileList;\n    currentCrawlIndex = 0;\n    cacheSize = 0;\n    clearProjectCache();\n    crawlEventSent = false;\n    cacheStartTime = Date.now();\n    currentCrawlID = currentCrawlID + 1;\n    fileCrawler(currentCrawlID);\n    WorkerComm.triggerPeer(\"crawlStarted\");\n    if(files.length === 0) {\n        WorkerComm.triggerPeer(\"crawlComplete\", {\n            numFilesCached: files.length,\n            cacheSizeBytes: 0,\n            crawlTimeMs: 0\n        });\n    }\n}\n\n/**\n * Remove the list of given files from the project cache\n * @param   {Object}   updateObject\n */\nfunction removeFilesFromCache(updateObject) {\n    console.log(\"file Indexer Document removed\", updateObject);\n    let fileList = updateObject.fileList || [],\n        filesInSearchScope = updateObject.filesInSearchScope || [],\n        i = 0;\n    for (i = 0; i < fileList.length; i++) {\n        delete projectCache[fileList[i]];\n    }\n    function isNotInRemovedFilesList(path) {\n        return (filesInSearchScope.indexOf(path) === -1) ? true : false;\n    }\n    files = files ? files.filter(isNotInRemovedFilesList) : files;\n}\n\n/**\n * Adds the list of given files to the project cache. However the files will not be\n * read at this time. We just delete the project cache entry which will trigger a fetch on search.\n * @param   {Object}   updateObject\n */\nfunction addFilesToCache(updateObject) {\n    console.log(\"file Indexer Document add\", updateObject);\n    let fileList = updateObject.fileList || [],\n        filesInSearchScope = updateObject.filesInSearchScope || [],\n        i = 0,\n        changedFilesAlreadyInList = [],\n        newFiles = [];\n    for (i = 0; i < fileList.length; i++) {\n        // We just add a null entry indicating the precense of the file in the project list.\n        // The file will be later read when required.\n        projectCache[fileList[i]] = null;\n    }\n\n    //Now update the search scope\n    function isInChangedFileList(path) {\n        return (filesInSearchScope.indexOf(path) !== -1) ? true : false;\n    }\n    changedFilesAlreadyInList = files ? files.filter(isInChangedFileList) : [];\n    function isNotAlreadyInList(path) {\n        return (changedFilesAlreadyInList.indexOf(path) === -1) ? true : false;\n    }\n    newFiles = changedFilesAlreadyInList.filter(isNotAlreadyInList);\n    files.push.apply(files, newFiles);\n}\n\n/**\n * Notification function on document changed, we update the cache with the contents\n * @param {Object} updateObject\n */\nfunction documentChanged(updateObject) {\n    console.log(\"documetn changed\", updateObject);\n    projectCache[updateObject.filePath] = updateObject.docContents;\n}\n\nfunction setTauriWsFS(nodeWSURL) {\n    fs.setNodeWSEndpoint(nodeWSURL);\n    fs.forceUseNodeWSEndpoint(true);\n}\n\nWorkerComm.setExecHandler(\"setTauriFSWS\", setTauriWsFS);\nWorkerComm.setExecHandler(\"initCache\", initCache);\nWorkerComm.setExecHandler(\"filesChanged\", addFilesToCache);\nWorkerComm.setExecHandler(\"documentChanged\", documentChanged);\nWorkerComm.setExecHandler(\"filesRemoved\", removeFilesFromCache);\n"],"file":"file-Indexing-Worker-thread.js"}