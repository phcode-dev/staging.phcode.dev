{"version":3,"sources":["worker/file-Indexing-Worker.js"],"names":["urlParams","URLSearchParams","location","search","debugMode","get","Phoenix","baseURL","importScripts","virtualfs","console","log","info","projectCache","files","MAX_FILE_SIZE_TO_INDEX","currentCrawlIndex","crawlComplete","crawlEventSent","cacheStartTime","Date","now","cacheSize","clearProjectCache","_statAsync","path","Promise","resolve","reject","fs","stat","err","stats","_readFileAsync","readFile","data","async","getFilesizeInBytes","fileName","size","ex","getFileContentsForFile","filePath","fileSize","fileCrawler","length","setTimeout","parallelRead","readPromises","i","push","contents","all","content","crawlTime","WorkerComm","triggerPeer","numFilesCached","cacheSizeBytes","crawlTimeMs","_crawlProgressMessenger","processed","total","initCache","fileList","removeFilesFromCache","updateObject","filesInSearchScope","isNotInRemovedFilesList","indexOf","filter","addFilesToCache","changedFilesAlreadyInList","newFiles","isInChangedFileList","isNotAlreadyInList","apply","documentChanged","docContents","setInterval","setExecHandler"],"mappings":"AAuBA,MAAMA,UAAY,IAAIC,gBAAgBC,SAASC,QACzCC,UAAwC,SAA3BJ,UAAUK,IAAI,SAE3BC,QAAU,CAEZC,QAAS,OAEbC,cAAc,2BACdA,cAAc,+BACdA,cAAc,mBAEdC,UAAUL,UAAYA,UAEtBM,QAAQC,IAAI,8CAA+CP,WAEvDA,YACAM,QAAQC,IAAMD,QAAQE,KAAO,cAKjC,IAAIC,aAAe,GACfC,MACAC,uBAAyB,SAEzBC,kBAAoB,EACpBC,eAAgB,EAChBC,gBAAiB,EACjBC,eAAiBC,KAAKC,MACtBC,UAAY,EAKhB,SAASC,oBACLV,aAAe,GAGnB,SAASW,WAAWC,MAChB,OAAO,IAAIC,QAAQ,CAACC,QAASC,UACzBC,GAAGC,KAAKL,KAAM,SAAUM,IAAKC,OACrBD,IACAH,OAAOG,KAEPJ,QAAQK,WAMxB,SAASC,eAAeR,MACpB,OAAO,IAAIC,QAAQ,CAACC,QAASC,UACzBC,GAAGK,SAAST,KAAM,OAAQ,SAAUM,IAAKI,MACjCJ,IACAH,OAAOG,KAEPJ,QAAQQ,UAYxBC,eAAeC,mBAAmBC,UAC9B,IACI,IAAIN,MACJ,aADkBR,WAAWc,WAChBC,MAAQ,EACvB,MAAOC,IAEL,OADA9B,QAAQC,IAAI6B,IACL,GAUfJ,eAAeK,uBAAuBC,UAClC,GAAI7B,aAAa6B,WAAwC,KAA3B7B,aAAa6B,UACvC,OAAO7B,aAAa6B,UAExB,IACI,IAAIC,eAAiBN,mBAAmBK,UAEpC7B,aAAa6B,UADZC,UAAY5B,6BACkBkB,eAAeS,UAErB,GAE/B,MAAOF,IACL9B,QAAQC,IAAI6B,IACZ3B,aAAa6B,UAAY,GAE7B,OAAO7B,aAAa6B,UAOxBN,eAAeQ,cACX,IAAK9B,OAAUA,OAA0B,IAAjBA,MAAM+B,OAE1B,YADAC,WAAWF,YAAa,KAG5B,MAAMG,aAAe,EACrB,IAAIC,aAAe,GACnB,IAAK,IAAIC,EAAI,EAAGA,EAFK,GAEejC,kBAAoBF,MAAM+B,OAAQI,IAClED,aAAaE,KAAKT,uBAAuB3B,MAAME,qBAC/CA,oBAEJ,IAAImC,eAAiBzB,QAAQ0B,IAAIJ,eAAiB,GAClD,IAAI,IAAIK,WAAWF,SACZE,SAAWA,QAAQR,SAClBvB,WAAa+B,QAAQR,QAG7B,GAAI7B,kBAAoBF,MAAM+B,OAC1B5B,eAAgB,EAChB6B,WAAWF,iBACR,CAEH,GADA3B,eAAgB,GACXC,eAAgB,CACjBA,gBAAiB,EACjB,IAAIoC,UAAalC,KAAKC,MAAQF,eAC9BoC,WAAWC,YAAY,gBAAiB,CACpCC,eAAgB3C,MAAM+B,OACtBa,eAAgBpC,UAChBqC,YAAaL,YAGrBR,WAAWF,YAAa,MAIhC,SAASgB,2BACD3C,eAAiBH,OACjByC,WAAWC,YAAY,gBAAiB,CACpCK,UAAW7C,kBACX8C,MAAOhD,MAAM+B,SAYzB,SAASkB,UAAUC,UACftD,QAAQC,IAAI,2CAA4CqD,SAASnB,QACjE/B,MAAQkD,SACRhD,kBAAoB,EACpBM,UAAY,EACZC,oBACAL,gBAAiB,EACjBC,eAAiBC,KAAKC,MACtBkC,WAAWC,YAAY,gBAO3B,SAASS,qBAAqBC,cAC1BxD,QAAQC,IAAI,gCAAiCuD,cAC7C,IAAIF,SAAWE,aAAaF,UAAY,GACpCG,mBAAqBD,aAAaC,oBAAsB,GACxDlB,EAAI,EACR,IAAKA,EAAI,EAAGA,EAAIe,SAASnB,OAAQI,WACtBpC,aAAamD,SAASf,IAEjC,SAASmB,wBAAwB3C,MAC7B,OAA8C,IAAtC0C,mBAAmBE,QAAQ5C,MAEvCX,MAAQA,MAAQA,MAAMwD,OAAOF,yBAA2BtD,MAQ5D,SAASyD,gBAAgBL,cACrBxD,QAAQC,IAAI,4BAA6BuD,cACzC,IAAIF,SAAWE,aAAaF,UAAY,GACpCG,mBAAqBD,aAAaC,oBAAsB,GACxDlB,EAAI,EACJuB,0BAA4B,GAC5BC,SAAW,GACf,IAAKxB,EAAI,EAAGA,EAAIe,SAASnB,OAAQI,IAG7BpC,aAAamD,SAASf,IAAM,KAIhC,SAASyB,oBAAoBjD,MACzB,OAA8C,IAAtC0C,mBAAmBE,QAAQ5C,MAGvC,SAASkD,mBAAmBlD,MACxB,OAAqD,IAA7C+C,0BAA0BH,QAAQ5C,MAE9CgD,UAJAD,0BAA4B1D,MAAQA,MAAMwD,OAAOI,qBAAuB,IAInCJ,OAAOK,oBAC5C7D,MAAMoC,KAAK0B,MAAM9D,MAAO2D,UAO5B,SAASI,gBAAgBX,cACrBxD,QAAQC,IAAI,mBAAoBuD,cAChCrD,aAAaqD,aAAaxB,UAAYwB,aAAaY,YAxEvDC,YAAYnB,wBAAyB,KA2ErCL,WAAWyB,eAAe,YAAajB,WACvCR,WAAWyB,eAAe,eAAgBT,iBAC1ChB,WAAWyB,eAAe,kBAAmBH,iBAC7CtB,WAAWyB,eAAe,eAAgBf,sBAE1CnB,WAAWF,YAAa","sourcesContent":["/*\n * GNU AGPL-3.0 License\n *\n * Copyright (c) 2021 - present core.ai . All rights reserved.\n * Original work Copyright (c) 2015 - 2021 Adobe Systems Incorporated. All rights reserved.\n *\n * This program is free software: you can redistribute it and/or modify it\n * under the terms of the GNU Affero General Public License as published by\n * the Free Software Foundation, either version 3 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful, but WITHOUT\n * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n * FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License\n * for more details.\n *\n * You should have received a copy of the GNU Affero General Public License\n * along with this program. If not, see https://opensource.org/licenses/AGPL-3.0.\n *\n */\n\n/*global virtualfs, fs, WorkerComm */\n\nconst urlParams = new URLSearchParams(location.search);\nconst debugMode = (urlParams.get('debug') === 'true');\n// eslint-disable-next-line no-unused-vars\nconst Phoenix = {\n    // exported to be used by extensions that extend the indexing worker\n    baseURL: '../'\n};\nimportScripts('../phoenix/virtualfs.js');\nimportScripts('../utils/EventDispatcher.js');\nimportScripts('./WorkerComm.js');\n\nvirtualfs.debugMode = debugMode;\n\nconsole.log(\"File indexing worker loaded in debug mode: \", debugMode);\n\nif(!debugMode){\n    console.log = console.info = function () {\n        // swallow logs\n    };\n}\n\nlet projectCache = [],\n    files,\n    MAX_FILE_SIZE_TO_INDEX = 16777216;\n\nlet currentCrawlIndex = 0,\n    crawlComplete = false,\n    crawlEventSent = false,\n    cacheStartTime = Date.now(),\n    cacheSize = 0;\n\n/**\n * Clears the cached file contents of the project\n */\nfunction clearProjectCache() {\n    projectCache = [];\n}\n\nfunction _statAsync(path) {\n    return new Promise((resolve, reject)=>{\n        fs.stat(path, function (err, stats) {\n            if (err) {\n                reject(err);\n            } else {\n                resolve(stats);\n            }\n        });\n    });\n}\n\nfunction _readFileAsync(path) {\n    return new Promise((resolve, reject)=>{\n        fs.readFile(path, 'utf8', function (err, data) {\n            if (err) {\n                reject(err);\n            } else {\n                resolve(data);\n            }\n        });\n    });\n}\n\n\n/**\n * Gets the file size in bytes.\n * @param   {string} fileName The name of the file to get the size\n * @returns {Number} the file size in bytes\n */\nasync function getFilesizeInBytes(fileName) {\n    try {\n        let stats = await _statAsync(fileName);\n        return stats.size || 0;\n    } catch (ex) {\n        console.log(ex);\n        return 0;\n    }\n}\n\n/**\n * Get the contents of a file from cache given the path. Also adds the file contents to cache from disk if not cached.\n * Will not read/cache files greater than MAX_FILE_SIZE_TO_INDEX in size.\n * @param   {string} filePath full file path\n * @return {string} contents or null if no contents\n */\nasync function getFileContentsForFile(filePath) {\n    if (projectCache[filePath] || projectCache[filePath] === \"\") {\n        return projectCache[filePath];\n    }\n    try {\n        let fileSize = await getFilesizeInBytes(filePath);\n        if ( fileSize <= MAX_FILE_SIZE_TO_INDEX) {\n            projectCache[filePath] = await _readFileAsync(filePath);\n        } else {\n            projectCache[filePath] = \"\";\n        }\n    } catch (ex) {\n        console.log(ex);\n        projectCache[filePath] = \"\";\n    }\n    return projectCache[filePath];\n}\n\n/**\n * Crawls through the files in the project ans stores them in cache. Since that could take a while\n * we do it in batches so that node wont be blocked.\n */\nasync function fileCrawler() {\n    if (!files || (files && files.length === 0)) {\n        setTimeout(fileCrawler, 1000);\n        return;\n    }\n    const parallelRead = 5;\n    let readPromises = [];\n    for (let i = 0; i < parallelRead && currentCrawlIndex < files.length; i++) {\n        readPromises.push(getFileContentsForFile(files[currentCrawlIndex]));\n        currentCrawlIndex++;\n    }\n    let contents = await Promise.all(readPromises) || [];\n    for(let content of contents){\n        if(content && content.length){\n            cacheSize += content.length;\n        }\n    }\n    if (currentCrawlIndex < files.length) {\n        crawlComplete = false;\n        setTimeout(fileCrawler);\n    } else {\n        crawlComplete = true;\n        if (!crawlEventSent) {\n            crawlEventSent = true;\n            let crawlTime =  Date.now() - cacheStartTime;\n            WorkerComm.triggerPeer(\"crawlComplete\", {\n                numFilesCached: files.length,\n                cacheSizeBytes: cacheSize,\n                crawlTimeMs: crawlTime\n            });\n        }\n        setTimeout(fileCrawler, 1000);\n    }\n}\n\nfunction _crawlProgressMessenger() {\n    if(!crawlComplete && files){\n        WorkerComm.triggerPeer(\"crawlProgress\", {\n            processed: currentCrawlIndex,\n            total: files.length\n        });\n    }\n}\n\nsetInterval(_crawlProgressMessenger, 1000);\n\n/**\n * Init for project, resets the old project cache, and sets the crawler function to\n * restart the file crawl\n * @param   {array} fileList an array of files\n */\nfunction initCache(fileList) {\n    console.log(\"file indexer: InitCache with num files: \", fileList.length);\n    files = fileList;\n    currentCrawlIndex = 0;\n    cacheSize = 0;\n    clearProjectCache();\n    crawlEventSent = false;\n    cacheStartTime = Date.now();\n    WorkerComm.triggerPeer(\"crawlStarted\");\n}\n\n/**\n * Remove the list of given files from the project cache\n * @param   {Object}   updateObject\n */\nfunction removeFilesFromCache(updateObject) {\n    console.log(\"file Indexer Document removed\", updateObject);\n    let fileList = updateObject.fileList || [],\n        filesInSearchScope = updateObject.filesInSearchScope || [],\n        i = 0;\n    for (i = 0; i < fileList.length; i++) {\n        delete projectCache[fileList[i]];\n    }\n    function isNotInRemovedFilesList(path) {\n        return (filesInSearchScope.indexOf(path) === -1) ? true : false;\n    }\n    files = files ? files.filter(isNotInRemovedFilesList) : files;\n}\n\n/**\n * Adds the list of given files to the project cache. However the files will not be\n * read at this time. We just delete the project cache entry which will trigger a fetch on search.\n * @param   {Object}   updateObject\n */\nfunction addFilesToCache(updateObject) {\n    console.log(\"file Indexer Document add\", updateObject);\n    let fileList = updateObject.fileList || [],\n        filesInSearchScope = updateObject.filesInSearchScope || [],\n        i = 0,\n        changedFilesAlreadyInList = [],\n        newFiles = [];\n    for (i = 0; i < fileList.length; i++) {\n        // We just add a null entry indicating the precense of the file in the project list.\n        // The file will be later read when required.\n        projectCache[fileList[i]] = null;\n    }\n\n    //Now update the search scope\n    function isInChangedFileList(path) {\n        return (filesInSearchScope.indexOf(path) !== -1) ? true : false;\n    }\n    changedFilesAlreadyInList = files ? files.filter(isInChangedFileList) : [];\n    function isNotAlreadyInList(path) {\n        return (changedFilesAlreadyInList.indexOf(path) === -1) ? true : false;\n    }\n    newFiles = changedFilesAlreadyInList.filter(isNotAlreadyInList);\n    files.push.apply(files, newFiles);\n}\n\n/**\n * Notification function on document changed, we update the cache with the contents\n * @param {Object} updateObject\n */\nfunction documentChanged(updateObject) {\n    console.log(\"documetn changed\", updateObject);\n    projectCache[updateObject.filePath] = updateObject.docContents;\n}\n\nWorkerComm.setExecHandler(\"initCache\", initCache);\nWorkerComm.setExecHandler(\"filesChanged\", addFilesToCache);\nWorkerComm.setExecHandler(\"documentChanged\", documentChanged);\nWorkerComm.setExecHandler(\"filesRemoved\", removeFilesFromCache);\n\nsetTimeout(fileCrawler, 3000);\n"],"file":"file-Indexing-Worker.js"}