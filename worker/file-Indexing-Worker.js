const urlParams=new URLSearchParams(location.search),debugMode="true"===urlParams.get("debug"),Phoenix={baseURL:"../"};importScripts("../phoenix/virtualfs.js"),importScripts("../utils/EventDispatcher.js"),importScripts("./WorkerComm.js"),virtualfs.debugMode=debugMode,console.log("File indexing worker loaded in debug mode: ",debugMode),debugMode||(console.log=console.info=function(){});let projectCache=[],files,MAX_FILE_SIZE_TO_INDEX=16777216,currentCrawlIndex=0,crawlComplete=!1,crawlEventSent=!1,cacheStartTime=Date.now(),cacheSize=0;function clearProjectCache(){projectCache=[]}function _statAsync(path){return new Promise((resolve,reject)=>{fs.stat(path,function(err,stats){err?reject(err):resolve(stats)})})}function _readFileAsync(path){return new Promise((resolve,reject)=>{fs.readFile(path,"utf8",function(err,data){err?reject(err):resolve(data)})})}async function getFilesizeInBytes(fileName){try{let stats;return(await _statAsync(fileName)).size||0}catch(ex){return console.log(ex),0}}async function getFileContentsForFile(filePath){if(projectCache[filePath]||""===projectCache[filePath])return projectCache[filePath];try{let fileSize=await getFilesizeInBytes(filePath);projectCache[filePath]=fileSize<=MAX_FILE_SIZE_TO_INDEX?await _readFileAsync(filePath):""}catch(ex){console.log(ex),projectCache[filePath]=""}return projectCache[filePath]}async function fileCrawler(){if(!files||files&&0===files.length)return void setTimeout(fileCrawler,1e3);const parallelRead=5;let readPromises=[];for(let i=0;i<5&&currentCrawlIndex<files.length;i++)readPromises.push(getFileContentsForFile(files[currentCrawlIndex])),currentCrawlIndex++;let contents=await Promise.all(readPromises)||[];for(let content of contents)content&&content.length&&(cacheSize+=content.length);if(currentCrawlIndex<files.length)crawlComplete=!1,setTimeout(fileCrawler);else{if(crawlComplete=!0,!crawlEventSent){crawlEventSent=!0;let crawlTime=Date.now()-cacheStartTime;WorkerComm.triggerPeer("crawlComplete",{numFilesCached:files.length,cacheSizeBytes:cacheSize,crawlTimeMs:crawlTime})}setTimeout(fileCrawler,1e3)}}function _crawlProgressMessenger(){!crawlComplete&&files&&WorkerComm.triggerPeer("crawlProgress",{processed:currentCrawlIndex,total:files.length})}function initCache(fileList){console.log("file indexer: InitCache with num files: ",fileList.length),files=fileList,currentCrawlIndex=0,cacheSize=0,clearProjectCache(),crawlEventSent=!1,cacheStartTime=Date.now(),WorkerComm.triggerPeer("crawlStarted")}function removeFilesFromCache(updateObject){console.log("file Indexer Document removed",updateObject);let fileList=updateObject.fileList||[],filesInSearchScope=updateObject.filesInSearchScope||[],i=0;for(i=0;i<fileList.length;i++)delete projectCache[fileList[i]];function isNotInRemovedFilesList(path){return-1===filesInSearchScope.indexOf(path)}files=files?files.filter(isNotInRemovedFilesList):files}function addFilesToCache(updateObject){console.log("file Indexer Document add",updateObject);let fileList=updateObject.fileList||[],filesInSearchScope=updateObject.filesInSearchScope||[],i=0,changedFilesAlreadyInList=[],newFiles=[];for(i=0;i<fileList.length;i++)projectCache[fileList[i]]=null;function isInChangedFileList(path){return-1!==filesInSearchScope.indexOf(path)}function isNotAlreadyInList(path){return-1===changedFilesAlreadyInList.indexOf(path)}newFiles=(changedFilesAlreadyInList=files?files.filter(isInChangedFileList):[]).filter(isNotAlreadyInList),files.push.apply(files,newFiles)}function documentChanged(updateObject){console.log("documetn changed",updateObject),projectCache[updateObject.filePath]=updateObject.docContents}setInterval(_crawlProgressMessenger,1e3),WorkerComm.setExecHandler("initCache",initCache),WorkerComm.setExecHandler("filesChanged",addFilesToCache),WorkerComm.setExecHandler("documentChanged",documentChanged),WorkerComm.setExecHandler("filesRemoved",removeFilesFromCache),setTimeout(fileCrawler,3e3);
//# sourceMappingURL=file-Indexing-Worker.js.map
